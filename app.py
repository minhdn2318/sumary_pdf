import os
import streamlit as st
import requests
import fitz
import docx
import faiss
import numpy as np
import gdown
from sentence_transformers import SentenceTransformer
from config import *

# =============================
# Utils
# =============================
def extract_pdf_text(file_path):
    """ƒê·ªçc text t·ª´ PDF"""
    text = ""
    try:
        doc = fitz.open(file_path)
        for page in doc:
            text += page.get_text("text") + "\n"
    except Exception as e:
        st.error(f"L·ªói ƒë·ªçc PDF: {e}")
    return text.strip()

def extract_docx_text(file_path):
    """ƒê·ªçc text t·ª´ DOCX"""
    try:
        doc = docx.Document(file_path)
        return "\n".join([p.text for p in doc.paragraphs]).strip()
    except Exception as e:
        st.error(f"L·ªói ƒë·ªçc DOCX: {e}")
        return ""

def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    """Chia nh·ªè text th√†nh c√°c chunk"""
    chunks = []
    if not text.strip():
        return chunks
    start = 0
    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks

# =============================
# FAISS Index
# =============================
def build_index(chunks, index_path=INDEX_PATH):
    """T·∫°o FAISS index t·ª´ list chunks"""
    if not chunks:
        return None, None, []
    model = SentenceTransformer(EMBEDDING_MODEL)
    embeddings = model.encode(chunks, convert_to_numpy=True)
    if embeddings.shape[0] == 0:
        return None, None, []
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)
    os.makedirs(os.path.dirname(index_path), exist_ok=True)
    faiss.write_index(index, index_path)
    return index, embeddings, chunks

def load_index(index_path=INDEX_PATH):
    if not os.path.exists(index_path):
        return None
    return faiss.read_index(index_path)

def search_chunks(query, chunks, index, top_k=TOP_K):
    """T√¨m top_k chunks li√™n quan nh·∫•t ƒë·∫øn query"""
    if index is None or not chunks:
        return []
    model = SentenceTransformer(EMBEDDING_MODEL)
    q_emb = model.encode([query], convert_to_numpy=True)
    D, I = index.search(q_emb, top_k)
    return [chunks[i] for i in I[0] if i < len(chunks)]

# =============================
# Call Groq API
# =============================
def ask_groq(chunks, question):
    if not chunks:
        return "‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ h·ªèi ƒë√°p."
    context = "\n\n".join(chunks)
    headers = {"Authorization": f"Bearer {GROQ_API_KEY}", "Content-Type": "application/json"}
    data = {
        "model": "gpt-4o-mini",
        "messages": [
            {"role": "system", "content": "B·∫°n l√† tr·ª£ l√Ω ƒë·ªçc hi·ªÉu t√†i li·ªáu."},
            {"role": "user", "content": f"Ng·ªØ c·∫£nh: {context}\n\nC√¢u h·ªèi: {question}"}
        ]
    }
    try:
        response = requests.post("{GROQ_API_URL}", headers=headers, json=data)
        if response.status_code == 200:
            return response.json()["choices"][0]["message"]["content"]
        else:
            return f"‚ùå L·ªói API: {response.status_code} - {response.text}"
    except Exception as e:
        return f"‚ùå L·ªói k·∫øt n·ªëi API: {e}"

# =============================
# Streamlit UI
# =============================
st.set_page_config(page_title="Tr·ª£ l√Ω T√†i li·ªáu", page_icon="üìö", layout="wide")
st.title("üìö Tr·ª£ l√Ω H·ªèi ƒë√°p T√†i li·ªáu (Grok API + FAISS)")

mode = st.radio("Ch·ªçn ngu·ªìn d·ªØ li·ªáu:", ["Google Drive (m·∫∑c ƒë·ªãnh)", "Upload th·ªß c√¥ng"])

if st.button("üîÑ ƒê·ªìng b·ªô l·∫°i OCR d·ªØ li·ªáu"):
    st.info("‚è≥ ƒêang ƒë·ªìng b·ªô l·∫°i d·ªØ li·ªáu...")

    os.makedirs("data", exist_ok=True)
    all_text = ""

    if mode == "Google Drive (m·∫∑c ƒë·ªãnh)":
        try:
            st.write("üì• ƒêang t·∫£i file t·ª´ Google Drive...")
            gdown.download_folder(GOOGLE_DRIVE_FOLDER, output="data", quiet=False, use_cookies=False)
            for file_name in os.listdir("data"):
                file_path = os.path.join("data", file_name)
                if file_path.endswith(".pdf"):
                    all_text += extract_pdf_text(file_path) + "\n"
                elif file_path.endswith(".docx"):
                    all_text += extract_docx_text(file_path) + "\n"
        except Exception as e:
            st.error(f"‚ùå L·ªói t·∫£i file t·ª´ Google Drive: {e}")

    else:
        uploaded_files = st.file_uploader("T·∫£i file PDF/DOCX", type=["pdf", "docx"], accept_multiple_files=True)
        if uploaded_files:
            for uploaded_file in uploaded_files:
                file_path = os.path.join("data", uploaded_file.name)
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                if file_path.endswith(".pdf"):
                    all_text += extract_pdf_text(file_path) + "\n"
                elif file_path.endswith(".docx"):
                    all_text += extract_docx_text(file_path) + "\n"

    chunks = chunk_text(all_text)
    if not chunks:
        st.error("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y text trong file. C√≥ th·ªÉ file to√†n ·∫£nh scan ho·∫∑c r·ªóng.")
    else:
        index, embeddings, saved_chunks = build_index(chunks)
        if index is None:
            st.error("‚ö†Ô∏è Kh√¥ng t·∫°o ƒë∆∞·ª£c FAISS index.")
        else:
            np.save("index/chunks.npy", saved_chunks)
            st.success(f"‚úÖ ƒê·ªìng b·ªô th√†nh c√¥ng! ({len(saved_chunks)} chunks)")

# =============================
# H·ªèi ƒë√°p
# =============================
question = st.text_area("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n", height=100)
if st.button("üöÄ H·ªèi t√†i li·ªáu"):
    if not os.path.exists(INDEX_PATH) or not os.path.exists("index/chunks.npy"):
        st.error("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu, h√£y ƒë·ªìng b·ªô tr∆∞·ªõc.")
    else:
        index = load_index()
        chunks = np.load("index/chunks.npy", allow_pickle=True)
        relevant_chunks = search_chunks(question, chunks.tolist(), index)
        answer = ask_groq(relevant_chunks, question)
        st.subheader("üí° Tr·∫£ l·ªùi:")
        st.write(answer)
