import os
import streamlit as st
import requests
import fitz
import docx
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from config import *

# =============================
# Utils
# =============================
def extract_pdf_text(file_path):
    text = ""
    doc = fitz.open(file_path)
    for page in doc:
        text += page.get_text("text") + "\n"
    return text

def extract_docx_text(file_path):
    doc = docx.Document(file_path)
    return "\n".join([p.text for p in doc.paragraphs])

def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    chunks = []
    start = 0
    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks

# =============================
# FAISS Index
# =============================
def build_index(chunks, index_path=INDEX_PATH):
    model = SentenceTransformer(EMBEDDING_MODEL)
    embeddings = model.encode(chunks, convert_to_numpy=True)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)
    faiss.write_index(index, index_path)
    return index, embeddings, chunks

def load_index(index_path=INDEX_PATH):
    if not os.path.exists(index_path):
        return None, None
    index = faiss.read_index(index_path)
    return index

def search_chunks(query, chunks, index, top_k=TOP_K):
    model = SentenceTransformer(EMBEDDING_MODEL)
    q_emb = model.encode([query], convert_to_numpy=True)
    D, I = index.search(q_emb, top_k)
    return [chunks[i] for i in I[0]]

# =============================
# Call Groq API
# =============================
def ask_groq(chunks, question):
    context = "\n\n".join(chunks)
    headers = {"Authorization": f"Bearer {GROQ_API_KEY}", "Content-Type": "application/json"}
    data = {
        "model": "gpt-4o-mini",
        "messages": [
            {"role": "system", "content": "Báº¡n lÃ  trá»£ lÃ½ Ä‘á»c hiá»ƒu tÃ i liá»‡u."},
            {"role": "user", "content": f"Ngá»¯ cáº£nh: {context}\n\nCÃ¢u há»i: {question}"}
        ]
    }
    response = requests.post(GROQ_API_URL, headers=headers, json=data)
    if response.status_code == 200:
        return response.json()["choices"][0]["message"]["content"]
    else:
        return f"âŒ Lá»—i API: {response.status_code} - {response.text}"

# =============================
# Streamlit UI
# =============================
st.set_page_config(page_title="Trá»£ lÃ½ TÃ i liá»‡u", page_icon="ðŸ“š", layout="wide")
st.title("ðŸ“š Trá»£ lÃ½ Há»i Ä‘Ã¡p TÃ i liá»‡u (Grok API + FAISS)")

mode = st.radio("Chá»n nguá»“n dá»¯ liá»‡u:", ["Google Drive (máº·c Ä‘á»‹nh)", "Upload thá»§ cÃ´ng"])

if st.button("ðŸ”„ Äá»“ng bá»™ láº¡i OCR dá»¯ liá»‡u"):
    st.info("Äang Ä‘á»“ng bá»™ láº¡i dá»¯ liá»‡u...")

    os.makedirs("data", exist_ok=True)

    all_text = ""

    if mode == "Google Drive (máº·c Ä‘á»‹nh)":
        # TODO: táº£i file tá»« Google Drive folder (cáº§n API key / pydrive / gdown)
        # VÃ­ dá»¥: báº¡n implement gdown.download_folder(GOOGLE_DRIVE_FOLDER)
        st.warning("ðŸš§ ChÆ°a implement láº¥y file tá»« Google Drive (cáº§n API Google Drive hoáº·c gdown).")
    else:
        uploaded_files = st.file_uploader("Táº£i file PDF/DOCX", type=["pdf", "docx"], accept_multiple_files=True)
        if uploaded_files:
            for uploaded_file in uploaded_files:
                file_path = os.path.join("data", uploaded_file.name)
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                if file_path.endswith(".pdf"):
                    all_text += extract_pdf_text(file_path)
                elif file_path.endswith(".docx"):
                    all_text += extract_docx_text(file_path)

    chunks = chunk_text(all_text)
    index, embeddings, saved_chunks = build_index(chunks)
    np.save("index/chunks.npy", saved_chunks)
    st.success("âœ… Äá»“ng bá»™ thÃ nh cÃ´ng!")

question = st.text_area("Nháº­p cÃ¢u há»i cá»§a báº¡n", height=100)
if st.button("ðŸš€ Há»i tÃ i liá»‡u"):
    if not os.path.exists(INDEX_PATH):
        st.error("âš ï¸ ChÆ°a cÃ³ dá»¯ liá»‡u, hÃ£y Ä‘á»“ng bá»™ trÆ°á»›c.")
    else:
        index = load_index()
        chunks = np.load("index/chunks.npy", allow_pickle=True)
        relevant_chunks = search_chunks(question, chunks, index)
        answer = ask_groq(relevant_chunks, question)
        st.subheader("ðŸ’¡ Tráº£ lá»i:")
        st.write(answer)
